# Postmortem: Digital Twin 3D sin Validación de Usuario

**Fecha:** 15 de marzo, 2023  
**Duración del proyecto:** 80 horas  
**Contexto:** R&D Technician - Establishment Labs  
**Resultado:** Feature rechazada por usuarios finales (12% adopción)

---

## Resumen Ejecutivo

Desarrollé visualización 3D interactiva (digital twin) de líneas de producción usando Three.js, integrando datos en tiempo real de sensores IoT. Invertí 80 horas antes de validar con operadores. Resultado: usuarios prefirieron dashboard tabular existente. Adopción final: 12%.

**Impacto:**
- 80 horas de desarrollo sin ROI
- 0% mejora en tiempo de diagnóstico
- Feature archivada 3 meses post-lanzamiento

**Aprendizaje clave:** Sofisticación técnica ≠ Valor para usuario

---

## ¿Qué sucedió?

### Contexto del Proyecto

**Problema percibido:**  
Operadores de planta diagnosticaban fallas en líneas de producción usando dashboard tabular. Asumí que visualización 3D sería más intuitiva.

**Solución propuesta:**  
Digital twin 3D mostrando:
- Layout físico de línea de producción
- Estado de cada máquina (verde/amarillo/rojo)
- Datos de sensores en tiempo real
- Interacción 3D: rotar, zoom, click para detalles

**Stack:**
- Three.js para renderizado 3D
- WebSocket para updates en tiempo real
- React para UI
- Node.js backend integrando sensores IoT

### Timeline

**Semana 1-2:** Diseño y prototipo estático  
**Semana 3-6:** Desarrollo del digital twin  
**Semana 7-8:** Integración con sensores IoT  
**Semana 9:** Testing interno (solo yo y supervisor)  
**Semana 10:** **Primera demo con operadores** → Feedback negativo  
**Semana 11-12:** Intentos de salvar el proyecto (fallidos)  
**Mes 3:** Feature archivada

---

## ¿Por qué falló?

### Asunciones Incorrectas

#### Asunción 1: "Visualización 3D es más intuitiva"

**Mi pensamiento:**
> "Ver la línea en 3D es más natural que tabla de números"

**Realidad de usuarios:**
> "La tabla me deja copiar/pegar valores para reportes. El 3D es bonito pero no puedo exportar datos."

**Insight:** Operadores priorizaban **funcionalidad** sobre **estética**.

#### Asunción 2: "Usuarios valoran innovación técnica"

**Mi pensamiento:**
> "Three.js + WebSockets en tiempo real impresionará"

**Realidad de usuarios:**
> "Es muy bonito, pero ¿cómo me ayuda a diagnosticar más rápido?"

**Insight:** Usuarios solo valoran features que resuelven dolor específico.

#### Asunción 3: "Si lo construyo, lo usarán"

**Mi pensamiento:**
> "80 horas de trabajo → Feature debe ser útil"

**Realidad de usuarios:**
> "Preferimos lo que ya conocemos. Cambiar workflow requiere justificación clara."

**Insight:** Switching cost es real, incluso para features "mejores".

---

## Feedback de Usuarios

### Demo con 8 operadores (Semana 10)

**Feedback positivo:**
- "Se ve muy profesional"
- "El 3D está bien logrado"
- "Impresionante que funcione en tiempo real"

**Feedback crítico:**
- "¿Puedo exportar estos datos a Excel?" → No
- "¿Puedo ver histórico de 24hrs?" → No (solo estado actual)
- "¿Puedo comparar dos máquinas lado a lado?" → No
- "La tabla me deja copiar valores, ¿el 3D también?" → No

**Pregunta clave de supervisor:**
> "¿Esto reduce el tiempo de diagnóstico de 8 minutos a cuánto?"

**Mi respuesta honesta:**
> "No lo medí. Asumí que visualización 3D sería más rápida."

**Resultado:** Feature percibida como "demo tech" sin valor operacional.

---

## Análisis de Causa Raíz

### 1. Falta de User Research

**Problema:** Nunca entrevisté a operadores antes de desarrollar.

**Debí preguntar:**
- ¿Qué parte del diagnóstico toma más tiempo?
- ¿Qué información necesitas que no tienes ahora?
- ¿Qué te frustra del dashboard actual?
- ¿Cómo usas los datos? (¿reportes? ¿análisis en vivo?)

**En su lugar:** Asumí problemas basándome en mi perspectiva (no la de ellos).

### 2. Solution-First Thinking

**Problema:** Empecé con solución ("digital twin") en lugar de problema validado.

**Flujo correcto:**
```
Problema validado → Validar solución con mockup → Desarrollar
```

**Mi flujo:**
```
Idea cool → Desarrollar 80hrs → Descubrir que no resuelve problema real
```

### 3. Enamored with Technology

**Problema:** Elegí Three.js porque quería aprender, no porque resolviera problema.

**Red flags:**
- "Será buena oportunidad para aprender WebGL"
- "Digital twins son tendencia en Industry 4.0"
- "Esto se verá increíble en mi portfolio"

**Ninguna de estas razones centra al usuario.**

### 4. No Testing Early Enough

**Problema:** Testing con usuarios en Semana 10 (después de 80hrs).

**Debí:** Testing en Semana 2 (después de 8hrs de mockup).

**Impacto:**
- Mockup + 5 entrevistas = 8hrs
- Habría descubierto rechazo sin invertir 80hrs

---

## Métricas de Fracaso

### Adopción

| Métrica | Target | Real |
|---------|--------|------|
| Adopción primeros 30 días | >80% | 12% |
| Tiempo en feature/día | >10 min | 45 segundos |
| Retención en 90 días | >60% | 3% |

### Performance vs Dashboard Existente

| Tarea | Dashboard Tabla | Digital Twin 3D |
|-------|----------------|-----------------|
| Diagnóstico de falla | 8 min | 8 min (sin mejora) |
| Exportar datos | 30 seg | N/A (no implementado) |
| Comparar máquinas | 1 min | 3 min (navegación 3D confusa) |

**Conclusión:** Digital twin no era objetivamente mejor en ninguna métrica.

---

## Lecciones Aprendidas

### ❌ Lo que NO funcionó

1. **Asumir que "más avanzado" = "mejor"**
   - Tabla simple > Visualización 3D compleja (para este caso de uso)
   - Simplicidad es un feature, no un bug

2. **Invertir 80hrs sin validación**
   - Sunk cost fallacy me hizo continuar aunque feedback fue claro
   - "Ya invertí mucho" no justifica seguir invirtiendo

3. **Enamorarse de la solución, no del problema**
   - Three.js era cool, pero no resolvía dolor real
   - Tech stack debe servir al usuario, no al ego del dev

### ✅ Lo que habría funcionado

1. **Fake Door Testing**
   - Mockup estático + 5 entrevistas = 8hrs
   - Detecta rechazo sin 80hrs de desarrollo

2. **Problem Interview First**
   - Entrevistar operadores: "¿Qué te frustra del dashboard?"
   - Validar problema antes de proponer solución

3. **MVP con feature mínimo viable**
   - Si insistía en 3D: versión básica en 20hrs
   - Validar adopción antes de añadir interactividad avanzada

4. **Métricas de éxito pre-definidas**
   - "Digital twin debe reducir tiempo de diagnóstico en 30%"
   - Si no cumple en 30 días, archivar feature

---

## Framework: "Show, Don't Build"

### Nueva Metodología (Post-Postmortem)

Basado en este fracaso, adopté framework:

#### Fase 1: Problem Discovery (2-4hrs)
- [ ] 5 entrevistas con usuarios
- [ ] Identificar top 3 dolores
- [ ] Cuantificar impacto (tiempo perdido, frustración)

#### Fase 2: Solution Validation (4-8hrs)
- [ ] Mockup estático (Figma/Sketch)
- [ ] Demo con 3-5 usuarios
- [ ] Medir interés: "¿Usarías esto diariamente?"
- [ ] **Gate:** Si <70% dice "sí" → Archivar o iterar

#### Fase 3: MVP Development (20-40hrs)
- [ ] Feature mínimo para validar hipótesis
- [ ] Instrumentar métricas de uso
- [ ] Beta con 10 usuarios durante 2 semanas

#### Fase 4: Go/No-Go Decision
- [ ] **Si adopción >60%:** Invertir en versión completa
- [ ] **Si adopción <60%:** Archivar o pivotar

**Resultado:** Invierte 30-50hrs para validar, no 80hrs para fracasar.

---

## Aplicación Futura

### Señales de alerta (Red Flags)

Si me encuentro pensando esto, STOP y validar primero:

- ❌ "Esto se verá increíble en mi portfolio"
- ❌ "Es buena oportunidad para aprender [tech nueva]"
- ❌ "Los usuarios no saben lo que necesitan"
- ❌ "Cuando vean el demo, lo amarán"

### Preguntas de validación

Antes de invertir >20hrs en feature:

1. ¿He entrevistado a 5+ usuarios del target?
2. ¿Puedo articular el problema en sus propias palabras?
3. ¿Tengo evidencia cuantitativa del dolor? (tiempo, costo, frustración)
4. ¿He mostrado mockup y obtuve >70% "usaría esto"?
5. ¿Tengo métricas de éxito pre-definidas?

**Si 1+ respuesta es "no" → No desarrollar aún.**

---

## Impacto en Portfolio

### Valor como Falla Documentada

Aunque el digital twin falló, este postmortem demuestra:

1. **Capacidad de autocrítica:** Reconozco errores sin excusas
2. **Aprendizaje profundo:** Extraje framework reusable
3. **User-centric thinking:** Ahora priorizo validación sobre tech
4. **Honestidad intelectual:** No escondo fracasos

**Paradoja:** Este "fracaso" es más valioso en portfolio que 10 proyectos "exitosos" sin postmortem.

---

## Recursos

- [The Mom Test](https://www.momtestbook.com/) - Rob Fitzpatrick  
  *Cómo hacer entrevistas de usuarios que no mienten*

- [Lean Startup Methodology](https://theleanstartup.com/) - Eric Ries  
  *Build-Measure-Learn loop*

- [Jobs To Be Done Framework](https://jtbd.info/)  
  *Entender por qué usuarios "contratan" features*

---

## Status

**Feature:** Archivada (Junio 2023)  
**Código:** Preservado como referencia técnica (Three.js learning)  
**Aprendizaje:** Aplicado en todos los proyectos subsecuentes  

**Resultado neto:** 80 horas invertidas compraron un framework que ha ahorrado 200+ horas en proyectos futuros.

---

**Última actualización:** Marzo 2023  
**Siguiente review:** Anual (evaluar si framework sigue vigente)
